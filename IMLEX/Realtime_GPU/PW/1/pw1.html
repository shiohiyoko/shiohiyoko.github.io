<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0"
    />
    <style>
      body {
        margin: 0;
        padding: 0;
        width: 100%;
        height: 100%;

        margin: 0;
        overflow: hidden;
        background-color: #aaaaaa;
        background-attachment: fixed !important;
      }
    </style>
    <style>
      body {
        font-family: Monospace;
        margin: 0px;
        overflow: hidden;
      }
    </style>
  </head>
  <body>
    <script
      async
      src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"
    ></script>
    <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.149.0/build/three.module.js",
          "three/addons/": "https://unpkg.com/three@0.149.0/examples/jsm/"
        }
      }
    </script>

    <script type="module">
		import * as THREE from "three";
		import { OrbitControls } from "three/addons/controls/OrbitControls.js";
		import WEBGL from "three/addons/capabilities/WebGL.js";
		import { VideoClass } from './VideoClass.js';

		/*
			Practical work 1
			Yuya Takagi
			3/21/2013

			This program is for color filtered anaglyph.
			We will have two stereo videos (San_Francisco.mp4 and moon.mp4) which is in the same directory

			These are the explanations for the js code used in this work
			- pw1.html
				Main function. This will cover the scene and rendering part of this program.
			- VideoClass.js
				Program which stores the main function for video processing. 
				This involves the VideoTexture, offscreen rendering, and additional gui settings 
				for anaglyph and filter parameters.
			- GUISettings.js
				Program which stores the seed for the GUI. Involves the basic stuctures of 
				uniform which will be passed to each offscreen rendering.
			- anaglyph.js
				Program which stores the functions and shader codes for offscreen rendering.
				
		*/

		var camera, controls, scene, renderer, container;
		var context, canvas;

		// stores the VideoClass
		let video;

		init();
		animate();

		// initialization function
		function init() {

			if (WEBGL.isWebGL2Available() === false) {
				document.body.appendChild(WEBGL.getWebGL2ErrorMessage());
			}

			container = document.createElement("div");
			document.body.appendChild(container);
			
			canvas = document.createElement("canvas");
			context = canvas.getContext("webgl2");
			document.body.appendChild(canvas);

			scene = new THREE.Scene();

			renderer = new THREE.WebGLRenderer({
				canvas: canvas,
				context: context,
			}); //, antialias: true, alpha: true } );
			renderer.autoClear = false;
			renderer.setPixelRatio(window.devicePixelRatio);
			renderer.setSize(window.innerWidth, window.innerHeight);
			renderer.shadowMap.enabled = false;

			container.appendChild(renderer.domElement);

			camera = new THREE.PerspectiveCamera(
				75,
				window.innerWidth / window.innerHeight,
				0.001,
				10
			);
			
			camera.position.z = 0.4;
			controls = new OrbitControls(camera, renderer.domElement);
			controls.minDistance = 0.005;
			controls.maxDistance = 2.0;
			controls.enableRotate = true;
			controls.addEventListener("change", render);
			controls.update();

			// controls for video
			video = new VideoClass(scene, document, canvas, context, 'San_Francisco.mp4');

			window.addEventListener("resize", onWindowResize, false);		
		}
		
		function render() {
			renderer.clear();
			video.checkIVprocess(renderer);
			renderer.render(scene, camera);
		}

		function animate() {
			requestAnimationFrame(animate);
			controls.update();
			render();
		}

		function onWindowResize() {
			camera.aspect = window.innerWidth / window.innerHeight;
			camera.updateProjectionMatrix();
			renderer.setSize(window.innerWidth, window.innerHeight);
			render();
		}
    </script>
  </body>
</html>
